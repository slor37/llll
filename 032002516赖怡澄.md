# 1.1Github链接
https://github.com/slor37/s/tree/slor37-032002516
---
# 一、PSP表格
| Personal Software Process Stages         | 预估耗时 （分钟） | 实际耗时 （分钟） |
| ---------------------------------------- | ----------------- | ----------------- |
| 计划                                     | 180               | 200               |
| · 估计这个任务需要多少时间               | 180               | 200               |
| 开发                                     | 10天              | 14天              |
| · 需求分析 (包括学习新技术)              | 5天               | 9天               |
| · 生成设计文档                           | 60                | 70                |
| · 设计复审                               | 10                | 5                 |
| · 代码规范 (为目前的开发制 定合适的规范) | 10                | 5                 |
| · 具体设计                               | 60                | 30                |
| · 具体编码                               | 2天               | 3天               |
| · 代码复审                               | 30                | 20                |
| · 测试（自我测试，修改代 码，提交修改）  | 1天               | 2天               |
| 报告                                     | 120               | 240               |
| · 测试报告                               | 60                | 120               |
| ·计算工作量                              | 30                | 60                |
| ·事后总结, 并提出过程改进 计划           | 30                | 60                |
| ·合计                                    | 12天              | 15天              |

# 二、任务要求的实现
## 项目设计与技术栈
### （2.1.1） 项目设计

> 这次任务主要分成三个大环节，第一步阅读理解题意，第二步实现代码，第三步完成报告。 其中第二步又分为两个环节，爬虫获取数据，数据可视化。
> 第二步爬虫获取数据，首先需要学习python语言与爬虫，学习主要是通过B站课程，当对python与爬虫有大体地了解后，开始尝试写代码。代码学习主要是参考csdn或者博客园或者简书或者知乎，参考研究大佬们的代码（因为爬虫基础为0），并模仿写代码。数据可视化学习与爬虫类似。
### （2.1.2） 技术栈
爬虫部分：

 - requests

 可视化部分：

 - pandas
 - numpy
 - matplotlib

## 爬虫与数据处理


> 主要用requests技术栈爬取数据
> def __init__(self) 
> self.area_url  
> self.home_url
> //默认构造函数,要爬的网站的地址
> def get_json_from_url(self, url)
> response = requests.get( url= url,headers = headers)
> //根据url,获取响应内容的字符串数据，设置header来隐藏自己，通过get方法向目标url发送get请求，返回响应的结果，是一个response对象
> def data_analyz(self,area_name,data)
> add_confirm = data['today']['confirm'] # 新增确诊
> //解析json内容，获取数据（新增确诊，累计确诊，累计治愈等）
> def get_countryAndcity_id(self)
> area_code_df = pd.DataFrame(area_code_list)
> return area_code_df
> //获取国家代码及城市代码，pd.DataFrame二维标签数据结构
> def get_area_yq_data_his(self,area_code,area_name)
> data_json = self.get_json_from_url(self.home_url+area_code)
> yq_json = data_json['data']['list']
> //获取不同代码的历史数据，并提取数据
> def list_with(self,Series)
> return ''.join(list(set(Series)))
> //去重
> def save_to_excel(self,df,sheetName)
> //生成Excel表格
> def get_res_code(self)
> //获取目标区域的id
> def run(self) 
> df = self.get_area_yq_data_his(area['area_code'],area['area_name'])
> //运行

## 数据统计接口部分的性能改进
使用cProfile分析


![在这里插入图片描述](https://img-blog.csdnimg.cn/dbbcbe72fe8f4fe0a7d8a65f24ab7b0c.jpeg#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/6a8fffc6aa76452eae83398e54bc6e3a.jpeg#pic_center)

改进：减少函数调用的数量，减少函数的调用层次
改进算法，如字典 (dictionary) 与列表 (list)，Python字典中使用了 hash table，因此，查找操作的复杂度为 O(1)，而 list 实际是个数组，在 list 中查找需要遍历整个 list，其复杂度为 O(n)，因此对成员的查找访问等操作字典要比 list 更快。



## 每日热点的实现思路

> 爬百度疫情话题
> 设headers，用requests技术栈去百度中筛选与疫情相关的新闻热点
> req = session.get(url, headers=headers)，
> 如果爬到，return req.text，将信息返回
> 再设一个函数def get_baidu(html)
> response = etree.HTML(html)
> //etree用来把得到的HTML对象，转变成属于lxml.etree._Element的类对象，调用etree的xpath()函数完成标签定位，此时获得的标签其实是xpath()函数返回的对象
> 创建txt文件保存筛选完的热点。

## 数据可视化界面的展示
![在这里插入图片描述](https://img-blog.csdnimg.cn/48141692a39849eab888412daf3353fa.jpeg#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/ff3231db8d7a4c2a838fc51ae7764f92.jpeg#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/809344da7f3f421d85a79f83007108f9.jpeg#pic_center)

> 可视化部分采用pandas、numpy、matplotlib技术栈。
> df = pd.read_excel("yq_data_all.xlsx") 
> // 用pandas读取疫情数据，并解析时间序列，索引设置
> df[df['area_name']=='中国']['add_confirm'].plot()
> //ploy函数绘制线图
> names = df['area_name'].value_counts().index.tolist()
> x = temp.index.tolist()
> y = temp.values
> //设置xy坐标
> plt.bar(x=x, height=y, label='数量', color='steelblue', alpha=0.8)
> //绘制柱状图


# 三、心得体会
这次作业对于我来说非常非常非常地难，因为完全是0基础，不会python，不会爬虫，不会数据可视化，什么都不会。一切都要从头学习，花了九天的时间在b战速成了爬虫（依然非常不熟练），在csdn和博客园参考阅读理解了大佬们爬虫代码与数据可视化代码，才勉强写出作业，这次的作业难度非常大，学的非常崩溃，但也提高了自学能力，对python，爬虫，可视化也有了初步掌握与理解，有所收获。